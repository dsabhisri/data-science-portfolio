{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "604ebce1",
   "metadata": {},
   "source": [
    "## Project Milestone 2 - Refine Your Project\n",
    "### Five prompt experiments to see if your problem has the potential to be solved using the model you chose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb45edd",
   "metadata": {},
   "source": [
    "## Notebook overview\n",
    "\n",
    "This notebook runs a set of prompt experiments to validate an LLM-based approach for your project (Project Milestone 2).\n",
    "\n",
    "How to run\n",
    "- Run CELL 2 first to initialize the OpenAI client and model configuration.\n",
    "- Then run experimental cells in order (CELL 3 ‚Üí CELL 4 ‚Üí CELL 5 ‚Üí CELL 6) or run individual experiment cells after the client is initialized.\n",
    "- Do not re-import or reassign shared variables (client, MODEL_NAME, MAX_TOKENS, TEMPERATURE) unless you intend to overwrite them.\n",
    "\n",
    "What each cell does\n",
    "- CELL 0: Notebook title / header.\n",
    "- CELL 2: Initializes OpenAI client and defines query_gpt helper and model config.\n",
    "- CELL 3: Experiment 1 ‚Äî code Q&A using a simulated code context.\n",
    "- CELL 4: ask_gpt helper function used to call the client.\n",
    "- CELL 5: Experiment 3 ‚Äî onboarding guidance using simple_context.\n",
    "- CELL 6: Experiment 5 ‚Äî debugging guidance using simple_debugging.\n",
    "\n",
    "Key variables (do not hardcode secrets in shared repos)\n",
    "- MODEL_NAME, MAX_TOKENS, TEMPERATURE: model configuration.\n",
    "- client: initialized OpenAI client.\n",
    "- Note: API keys are currently set in variables; move them to environment variables or a secrets manager before committing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1bc67a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI client initialized successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "OPENAI_API_KEY1 = \"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\"\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY1)\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "TEMPERATURE = 0.3  # Lower temperature for more consistent, factual responses\n",
    "MAX_TOKENS = 500\n",
    "\n",
    "# Helper function for chat completions\n",
    "def query_gpt(system_prompt, user_prompt, context=\"\"):\n",
    "    \"\"\"\n",
    "    Query GPT-3.5-Turbo with system prompt, context, and user query.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt}\n",
    "    ]\n",
    "    \n",
    "    if context:\n",
    "        messages.append({\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Context:\\n{context}\\n\\nQuestion: {user_prompt}\"\n",
    "        })\n",
    "    else:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_TOKENS\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"‚úÖ OpenAI client initialized successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64da8b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXPERIMENT 1: Direct Code Q&A (Function Documentation)\n",
      "======================================================================\n",
      "\n",
      "üìù PROMPT:\n",
      "Based on the code provided, explain what the authenticate_user() function does.\n",
      "Include:\n",
      "1. What the function does in 1-2 sentences\n",
      "2. What each parameter represents\n",
      "3. What it returns and under what conditions\n",
      "4. Any important side effects\n",
      "5. A usage example\n",
      "\n",
      "Reference the specific file and line numbers.\n",
      "\n",
      "ü§ñ MODEL RESPONSE:\n",
      "1. The `authenticate_user()` function in auth_service.py (lines 45-67) authenticates a user by checking the provided username and password against a stored hash in the user database.\n",
      "\n",
      "2. Parameters:\n",
      "   - `username`: Represents the user's login identifier (email or username).\n",
      "   - `password`: Represents the plaintext password to verify against the stored hash.\n",
      "\n",
      "3. Returns:\n",
      "   - Returns True if the authentication succeeds, indicating valid credentials.\n",
      "   - Returns False if the authentication fails, indicating invalid credentials.\n",
      "\n",
      "4. Side Effects:\n",
      "   - Logs authentication attempts to the `audit_log` table.\n",
      "   - Increments the `failed_attempts` counter in the user database (reset on successful authentication).\n",
      "   - Raises a ValueError if there is a database connection failure.\n",
      "\n",
      "5. Usage Example:\n",
      "```python\n",
      "result = authenticate_user(\"example_user\", \"password123\")\n",
      "if result:\n",
      "    print(\"Authentication successful!\")\n",
      "else:\n",
      "    print(\"Invalid credentials. Authentication failed.\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: Code Understanding\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 1: Direct Code Q&A (Function Documentation)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simulated retrieved context (in real system, this comes from vector DB)\n",
    "code_context = \"\"\"\n",
    "File: auth_service.py, lines 45‚Äì67\n",
    "\n",
    "def authenticate_user(username: str, password: str) -> bool:\n",
    "    '''Authenticate a user against the configured user database.\n",
    "    \n",
    "    Args:\n",
    "        username: The user's login identifier (email or username).\n",
    "        password: The plaintext password to verify against stored hash.\n",
    "    \n",
    "    Returns:\n",
    "        True if authentication succeeds; False if credentials are invalid.\n",
    "    \n",
    "    Side effects:\n",
    "        - Logs authentication attempts to audit_log table\n",
    "        - Increments failed_attempts counter (reset on success)\n",
    "        - Raises ValueError if database connection fails\n",
    "    '''\n",
    "    try:\n",
    "        user = db.query(User).filter_by(username=username).first()\n",
    "        if not user:\n",
    "            log_auth_attempt(username, success=False)\n",
    "            return False\n",
    "        if verify_password(password, user.password_hash):\n",
    "            user.failed_attempts = 0\n",
    "            db.session.commit()\n",
    "            log_auth_attempt(username, success=True)\n",
    "            return True\n",
    "        else:\n",
    "            user.failed_attempts += 1\n",
    "            db.session.commit()\n",
    "            log_auth_attempt(username, success=False)\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Database error during authentication: {e}\")\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"\"\"You are an expert code documentation assistant. \n",
    "Explain code clearly and accurately based ONLY on the provided context. \n",
    "Always cite file names and line numbers. If information is not in the context, say so.\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"Based on the code provided, explain what the authenticate_user() function does.\n",
    "Include:\n",
    "1. What the function does in 1-2 sentences\n",
    "2. What each parameter represents\n",
    "3. What it returns and under what conditions\n",
    "4. Any important side effects\n",
    "5. A usage example\n",
    "\n",
    "Reference the specific file and line numbers.\"\"\"\n",
    "\n",
    "# Execute query\n",
    "response = query_gpt(system_prompt, user_prompt, code_context)\n",
    "\n",
    "print(\"\\nüìù PROMPT:\")\n",
    "print(user_prompt)\n",
    "print(\"\\nü§ñ MODEL RESPONSE:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bfc62e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENT 3: Onboarding Guidance\n",
      "======================================================================\n",
      "\n",
      "‚ùì QUESTION:\n",
      "I'm new to the team. I need to add email alerts when our null checks fail.\n",
      "Where should I put this code? What files should I look at first?\n",
      "\n",
      "ü§ñ ANSWER:\n",
      "To add email alerts when null checks fail, you should follow these steps:\n",
      "\n",
      "1. **Find the right folder for your feature**:\n",
      "   Since email alerts are notifications, you should add your code in the `/alerts/` folder.\n",
      "\n",
      "2. **Look at existing examples**:\n",
      "   Take a look at `/alerts/slack_alerts.py` to understand the pattern for sending notifications. All alert functions follow a similar structure.\n",
      "\n",
      "3. **Create your new file**:\n",
      "   - Name your new file as `email_alerts.py`.\n",
      "   - Copy the pattern from `slack_alerts.py` and modify it to send email alerts.\n",
      "\n",
      "4. **Connect it to your pipeline**:\n",
      "   - Import your email alert function in `/validation/check_nulls.py`.\n",
      "   - Call the email alert function when a null check fails.\n",
      "\n",
      "By following these steps, you can successfully add email alerts to the data pipeline when null checks fail. If you have any specific questions or need further guidance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "def ask_gpt(question, context):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful coding mentor. Guide developers using the provided codebase documentation.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Documentation:\\n{context}\\n\\nQuestion: {question}\"}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=400\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Simple documentation about the codebase\n",
    "simple_context = \"\"\"\n",
    "PROJECT STRUCTURE:\n",
    "\n",
    "/data_pipeline/\n",
    "  ‚îú‚îÄ‚îÄ ingestion/          # Read data from sources\n",
    "  ‚îú‚îÄ‚îÄ validation/         # Check data quality\n",
    "  ‚îÇ   ‚îú‚îÄ‚îÄ check_nulls.py\n",
    "  ‚îÇ   ‚îî‚îÄ‚îÄ check_duplicates.py\n",
    "  ‚îú‚îÄ‚îÄ transformation/     # Clean and transform data\n",
    "  ‚îî‚îÄ‚îÄ alerts/            # Send notifications\n",
    "      ‚îî‚îÄ‚îÄ slack_alerts.py\n",
    "\n",
    "HOW TO ADD A NEW FEATURE:\n",
    "\n",
    "1. Find the right folder for your feature\n",
    "   - Data checks? ‚Üí /validation/\n",
    "   - Notifications? ‚Üí /alerts/\n",
    "   - Transformations? ‚Üí /transformation/\n",
    "\n",
    "2. Look at existing examples\n",
    "   - /alerts/slack_alerts.py shows how to send Slack messages\n",
    "   - All alert functions follow this pattern:\n",
    "   \n",
    "   def send_alert(message, severity=\"INFO\"):\n",
    "       # Connect to notification service\n",
    "       # Format message\n",
    "       # Send notification\n",
    "       return success_status\n",
    "\n",
    "3. Create your new file\n",
    "   - Name it clearly (e.g., email_alerts.py)\n",
    "   - Copy the pattern from slack_alerts.py\n",
    "   - Change the notification method\n",
    "\n",
    "4. Connect it to your pipeline\n",
    "   - Import your function in validation/check_nulls.py\n",
    "   - Call it when a check fails:\n",
    "     if data_has_errors:\n",
    "         send_email_alert(\"Data quality issue found!\")\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"I'm new to the team. I need to add email alerts when our null checks fail.\n",
    "Where should I put this code? What files should I look at first?\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 3: Onboarding Guidance\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚ùì QUESTION:\\n{question}\")\n",
    "\n",
    "answer = ask_gpt(question, simple_context)\n",
    "\n",
    "print(f\"\\nü§ñ ANSWER:\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "322dafab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXPERIMENT 4: Design Decision Explanation\n",
      "======================================================================\n",
      "\n",
      "‚ùì QUESTION:\n",
      "Why did we choose PostgreSQL instead of MongoDB? \n",
      "I heard MongoDB is more modern and flexible.\n",
      "\n",
      "ü§ñ ANSWER:\n",
      "We chose PostgreSQL over MongoDB for our customer database for several reasons:\n",
      "\n",
      "1. **Clear Relationships**: Our data has clear relationships where customers are associated with multiple orders. PostgreSQL is a relational database that excels at handling such relationships efficiently.\n",
      "\n",
      "2. **Complex Queries**: We needed to perform complex queries for generating analytics reports and extracting insights from our data. PostgreSQL is known for its powerful query capabilities, making it a suitable choice for our use case.\n",
      "\n",
      "3. **Transactions**: Transactions are critical for us, especially in scenarios like payment processing where data consistency is crucial. PostgreSQL provides strong support for transactions, ensuring data integrity and consistency.\n",
      "\n",
      "4. **Team Familiarity**: Our team already has expertise in SQL, which is the query language used by PostgreSQL. This familiarity with SQL made it easier for our team to work with PostgreSQL effectively.\n",
      "\n",
      "While MongoDB is indeed more modern and flexible with its document-based approach and support for flexible schemas, we prioritized data relationships, complex queries, and transaction support for our specific requirements. By choosing PostgreSQL, we accepted the trade-off of giving up some flexibility in schema design in favor of strong consistency, relational capabilities, and transaction support.\n"
     ]
    }
   ],
   "source": [
    "# Simple architecture decision record\n",
    "simple_decision = \"\"\"\n",
    "DECISION: Use PostgreSQL for our customer database\n",
    "\n",
    "DATE: January 2024\n",
    "TEAM: Backend Engineering\n",
    "\n",
    "WHY WE NEEDED A DATABASE:\n",
    "- Store customer information (name, email, orders)\n",
    "- Need to query by customer ID, email, and order history\n",
    "- Must handle transactions (orders must be consistent)\n",
    "\n",
    "OPTIONS WE CONSIDERED:\n",
    "\n",
    "1. PostgreSQL (Relational Database)\n",
    "   ‚úì Good at: Relationships between data (customers ‚Üí orders)\n",
    "   ‚úì Good at: Complex queries (find all customers who ordered in last 30 days)\n",
    "   ‚úì Good at: Transactions (order + payment must succeed together)\n",
    "   ‚úó Bad at: Flexible schemas (adding fields requires migrations)\n",
    "   Cost: Free (open source)\n",
    "\n",
    "2. MongoDB (Document Database)\n",
    "   ‚úì Good at: Flexible schemas (easy to add new fields)\n",
    "   ‚úì Good at: Storing complex nested data\n",
    "   ‚úó Bad at: Relationships across collections\n",
    "   ‚úó Bad at: Complex transactions (weaker than PostgreSQL)\n",
    "   Cost: Free (open source) but more memory needed\n",
    "\n",
    "3. DynamoDB (AWS Managed NoSQL)\n",
    "   ‚úì Good at: Scaling automatically\n",
    "   ‚úì Good at: Simple key lookups\n",
    "   ‚úó Bad at: Complex queries (need to design around limitations)\n",
    "   ‚úó Bad at: Cost (expensive at scale)\n",
    "   Cost: ~$500/month at our scale\n",
    "\n",
    "WHAT WE CHOSE: PostgreSQL\n",
    "\n",
    "WHY:\n",
    "- Our data has clear relationships (customers have many orders)\n",
    "- We need complex queries (analytics reports)\n",
    "- Transactions are critical (payment processing)\n",
    "- Team already knows SQL\n",
    "\n",
    "TRADE-OFF WE ACCEPTED:\n",
    "- We gave up flexible schemas (MongoDB's strength)\n",
    "- We get strong consistency and relations (our priority)\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"Why did we choose PostgreSQL instead of MongoDB? \n",
    "I heard MongoDB is more modern and flexible.\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 4: Design Decision Explanation\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚ùì QUESTION:\\n{question}\")\n",
    "\n",
    "answer = ask_gpt(question, simple_decision)\n",
    "\n",
    "print(f\"\\nü§ñ ANSWER:\\n{answer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7685aa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXPERIMENT 5: Debugging Help\n",
      "======================================================================\n",
      "\n",
      "‚ùì QUESTION:\n",
      "Our process_orders.py script keeps crashing with \"MemoryError: unable to allocate array\".\n",
      "It works with small data but fails on production data. What's wrong?\n",
      "\n",
      "ü§ñ ANSWER:\n",
      "The issue with the `calculate_daily_totals` function in the `process_orders.py` script is that it loads all orders into memory at once using the `collect()` method. This approach is not efficient and leads to memory errors when processing a large amount of data.\n",
      "\n",
      "To address this issue and prevent memory errors, you can follow these steps:\n",
      "\n",
      "1. **Identify the Problem**:\n",
      "   - Loading all orders into memory at once is causing memory errors.\n",
      "   - The script works fine with small data but crashes with production data.\n",
      "\n",
      "2. **Debugging Steps**:\n",
      "   - Check memory usage to see if the process is consuming a large amount of RAM.\n",
      "   - Check the amount of data being processed to identify if it exceeds the system's capacity.\n",
      "\n",
      "3. **Fix Approaches**:\n",
      "   - **Option A - Use SQL Aggregation**:\n",
      "     - Instead of loading all data into memory, perform the aggregation directly in the database using SQL queries.\n",
      "     - This approach offloads the processing work to the database, which is more efficient for large datasets.\n",
      "  \n",
      "   - **Option B - Process in Batches**:\n",
      "     - Process the data in smaller batches instead of loading everything at once.\n",
      "     - You can chunk the data into manageable sizes and process each batch separately to avoid memory issues.\n",
      "  \n",
      "   - **Option C - Upgrade Server RAM**:\n",
      "     - Increasing the server's RAM may temporarily alleviate the issue, but it's not a recommended solution.\n",
      "     - It's better to optimize the code first before considering hardware upgrades.\n",
      "\n",
      "By implementing either Option A (SQL aggregation) or Option B (processing in batches), you can improve the efficiency of the script and avoid memory errors when processing large datasets.\n"
     ]
    }
   ],
   "source": [
    "# Simple debugging guide\n",
    "simple_debugging = \"\"\"\n",
    "CODE THAT'S FAILING:\n",
    "\n",
    "File: process_orders.py, line 42\n",
    "\n",
    "def calculate_daily_totals(orders_df):\n",
    "    # This loads ALL orders into memory at once\n",
    "    all_orders = orders_df.collect()  # ‚ö†Ô∏è PROBLEM: Loads everything into RAM\n",
    "    \n",
    "    daily_totals = {}\n",
    "    for order in all_orders:\n",
    "        date = order['order_date']\n",
    "        if date not in daily_totals:\n",
    "            daily_totals[date] = 0\n",
    "        daily_totals[date] += order['total_amount']\n",
    "    \n",
    "    return daily_totals\n",
    "\n",
    "SYSTEM CONFIGURATION:\n",
    "\n",
    "- Server RAM: 8 GB\n",
    "- Database: 5 million orders\n",
    "- Typical query: Processes 100,000 orders (about 2 GB of data)\n",
    "\n",
    "KNOWN ISSUES:\n",
    "\n",
    "1. collect() loads ALL data into memory\n",
    "   - Works fine with 10,000 orders (200 MB)\n",
    "   - Crashes with 100,000+ orders (2+ GB)\n",
    "   - Python dict operations slow on huge datasets\n",
    "\n",
    "2. Processing row-by-row is inefficient\n",
    "   - Better to use SQL aggregation (database does the work)\n",
    "   - Better to process in batches (chunk the data)\n",
    "\n",
    "DEBUGGING STEPS:\n",
    "\n",
    "1. Check memory usage\n",
    "   - Run: htop or top command\n",
    "   - Look for process using >80% RAM\n",
    "   - Check if process gets \"Killed\" (out of memory error)\n",
    "\n",
    "2. Check how much data you're processing\n",
    "   - Add: print(f\"Processing {orders_df.count()} orders\")\n",
    "   - If >50,000 orders, memory issues likely\n",
    "\n",
    "3. Fix approaches:\n",
    "\n",
    "   OPTION A - Use SQL aggregation (best):\n",
    "   query = '''\n",
    "       SELECT order_date, SUM(total_amount) as daily_total\n",
    "       FROM orders\n",
    "       GROUP BY order_date\n",
    "   '''\n",
    "   result = database.execute(query)\n",
    "   \n",
    "   OPTION B - Process in batches:\n",
    "   for batch in orders_df.to_batches(size=10000):\n",
    "       process_batch(batch)\n",
    "   \n",
    "   OPTION C - Add more RAM:\n",
    "   Upgrade server from 8GB ‚Üí 16GB\n",
    "   (Not recommended - fix code first!)\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"Our process_orders.py script keeps crashing with \"MemoryError: unable to allocate array\".\n",
    "It works with small data but fails on production data. What's wrong?\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 5: Debugging Help\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚ùì QUESTION:\\n{question}\")\n",
    "\n",
    "answer = ask_gpt(question, simple_debugging)\n",
    "\n",
    "print(f\"\\nü§ñ ANSWER:\\n{answer}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
